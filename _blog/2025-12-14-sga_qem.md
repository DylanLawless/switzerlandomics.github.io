---
title: Why evidence availability needs a standard
layout: page
math: mathjax
date: 2025-12-15
---

<p>{{ page.date | date: "%Y-%m-%d" }}</p>

Modern data systems are good at producing answers. They are much worse at stating what evidence was actually available when those answers were produced.

The [Qualifying Evidence Matrix (QEM)](https://www.swissgenomicsassociation.ch/assets/release/sga_qem/latest/sga_qem_1.0/) standard exists to solve this specific problem. It is not an interpretative or decision making framework. It is a normalisation standard for evidence availability.

In computer science terms, QEM plays a role similar to an [*intermediate representation (IR)*](https://en.wikipedia.org/wiki/Intermediate_representation), a stable **data structure** that sits between raw rule evaluation and downstream interpretation. Its purpose is simple: to record, in a fixed and interoperable way, whether verifiable evidence was present.

## A simple example

A student has no recorded result for an exam.

One teacher assumes the student failed. Another assumes the student was absent. A third assumes the grade was lost.

Now consider a fourth possibility. The student attended a private school in another country. The school has an excellent reputation, but it does not use a pass or fail grading system at all. 

If the register records only pass or fail, all four situations collapse into the same outcome. The difference between failure, non assessment, missing data, and incompatible grading disappears. Any later judgement becomes unreliable.

A proper exam register first records whether the exam was actually taken. Only then does a grade, in whatever system applies, have meaning.

Evidence systems face the same issue.


## Evaluation before interpretation

Many evidence rules cannot always be evaluated. Data may be missing or unavailable at the time of analysis.

QEM enforces a strict order. Each rule must first emit a raw outcome that distinguishes evaluated states, TRUE, FALSE, or NA. This follows the logic of [*three-valued logic*](https://en.wikipedia.org/wiki/Three-valued_logic), where NA represents not evaluable rather than true or false.

Only in a second step is this outcome mapped into a **binary** record of evidence availability, using standard [*binary*](https://en.wikipedia.org/wiki/Binary_number) or [*boolean*](https://en.wikipedia.org/wiki/Boolean_data_type) representations.

Skipping this step and writing a binary result directly loses essential information. QEM explicitly disallows this.

## Why separation matters

Ambiguity arises when identical outputs can mean different things. A negative result may reflect a true absence, or it may reflect that nothing was assessed at all.

QEM removes this ambiguity by separating rule outcomes from evidence availability. This mirrors a well established principle in [*formal systems*](https://en.wikipedia.org/wiki/Formal_system), where [*syntax*](https://en.wikipedia.org/wiki/Syntax) is handled before [*semantics*](https://en.wikipedia.org/wiki/Semantics), so meaning is constructed on stable ground.

QEM does not assess correctness, causality, or confidence. It performs *normalisation* only, using a fixed mapping from rule outcomes to availability, consistent with established [*data normalisation*](https://en.wikipedia.org/wiki/Database_normalization) principles.

Interpretation is deferred by design.

## No single existing standard fully addresses this problem.

In databases and programming languages, [*three-valued logic*](https://en.wikipedia.org/wiki/Three-valued_logic) and `NULL` semantics distinguish TRUE, FALSE, and unknown states. This captures non evaluability at the representation level, but does not mandate how such states are propagated, normalised, or audited across heterogeneous rule systems.

Data modelling standards and [*canonical data models*](https://en.wikipedia.org/wiki/Canonical_model) define structure, but assume field semantics are already agreed. They do not treat non evaluability or absence of evidence as first class information.

Provenance frameworks such as [*W3C PROV*](https://en.wikipedia.org/wiki/W3C_Prov) describe how data was generated or transformed, but do not define a minimal, deterministic normal form for evidence availability itself.

Clinical and regulatory standards, including [*HL7*](https://en.wikipedia.org/wiki/Health_Level_7), [*FHIR*](https://en.wikipedia.org/wiki/Fast_Healthcare_Interoperability_Resources), and [*openEHR*](https://en.wikipedia.org/wiki/OpenEHR), include domain specific markers such as “unknown” or “not done”. These conventions mix representation and interpretation and do not generalise across systems.

Frameworks for [*AI governance*](https://en.wikipedia.org/wiki/AI_governance) recognise the importance of missing or unevaluated evidence, but lack a low level, system independent representation that enforces this separation mechanically.

The gap is therefore not awareness, but the absence of a small, explicit, interoperable normal form whose sole purpose is to record whether verifiable evidence was present. This is the layer QEM defines.


## The consequence for trustworthy systems

Many failures in [*artificial intelligence (AI)*](https://en.wikipedia.org/wiki/Artificial_intelligence) and data-driven decision making do not stem from incorrect models. They stem from uncertainty about what information the system actually had access to.

Without an explicit record of evidence availability, absence is silently treated as evidence. Gaps are misread as conclusions.

QEM provides a minimal but essential layer. It ensures that any downstream interpretation rests on a clear, auditable account of whether the required evidence was present.

It does not make systems trustworthy by itself. It makes trust claims structurally complete.

